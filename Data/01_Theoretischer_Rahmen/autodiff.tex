% https://en.wikipedia.org/wiki/Automatic_differentiation
% https://www.youtube.com/watch?v=wG_nF1awSSY

\section{Automatisches Differenzieren} \label{sec:auto_diffentiation}

Beim automatischen Differenzieren \cite[Kapitel~2]{autodiff} geht darum, die Ableitung einer Funktion automatisch zu berechnen.
Das Besondere dabei ist, das diese meistens als Computerprogramm gegeben ist.
Automatisches Differenzieren findet Anwendung beim Trainieren von neuronalen Netzen 
und dem Lösen von nicht-linearen Gleichungssystemen durch das Newton-Verfahren.
Der zweite Grund ist für diese Arbeit besonders relevant, da 
dieses benötigt wird, um die Runge-Kutta-Verfahren, die in Kapitel \ref{sec:runge_kutta} vorgestellt wurden, zu berechnen.
Die mathematische Definition der Funktion lautet wie folgt:

$$
f: \mathbb{R}^n \rightarrow \mathbb{R}^m, x \mapsto y
$$

Gesucht werden nun die partiellen Ableitungen bzw. die Jacobo-Matrix von $f$:

$$
\frac{\partial f}{\partial x} = \left[ \frac{\partial y_i}{\partial x_i} \right]_{i=1..m, j=1..n} 
$$

Die Jacobi-Matrix wird berechnet, wenn alle partiellen Ableitungen einer Funktion benötigt werden.
Wird nur eine konkrete Partielle-Ableitung benötigt, kann diese ebenfalls mit Autodifferenzierung berechnet werden.

\subsection{Implementation} \label{sec:implementation}

Es gibt grundsätzlich vier Wege automatische Differenzierung umzusetzen.
Die einfachste Variante approximiert die Limitdefinition, indem sie $h$ auf einen kleinen Wert setzt.

$$
\frac{\partial f_k}{\partial x} = \lim_{h \to 0} \frac{f_k(x+ h) - f_k(x)}{h} \approx \frac{f_k(x + h) - f_k(x)}{h} 
$$

Diese Methode ist sehr leicht zu implementieren, allerdings ist sie sehr ungenau und instabil.

Eine exakte Lösung ergibt sich mit der Verwendung eines Computer-Algebra-Systems wie zum Beispiel Sympy \cite{sympy}.
Dieses Verfahren operiert auf der symbolischen Repräsentation der Funktion.
Es verwendet also die unterschiedlichen Ableitungs-Regeln, um eine exakte Lösung zu berechnen.
Dieses Verfahren hat das Problem, dass häufig verwendete Ableitungsregel wie zum Beispiel die Produktregel
mit jeder Anwendung die Ergebniss-Größe verdoppelt, wodurch es zu einem exponentiellen Wachstum 
der symbolischen Repräsentation kommt.
Dies macht dieses Verfahren unbrauchbar, um die Ableitung eines neuronalen Netzes zu berechnen.

Die dritte Methode nutzt die Möglichkeit, aus Operationen 
in einer Programmiersprache überladen zu können.
Es wird dazu angenommen, dass die Funktion, dessen Ableitung berechnet werden soll, aus primitiven Operationen aufgebaut ist, dessen Ableitung bekannt ist.
Nun kann die Kettenregel ausgenutzt werden, um Schritt für Schritt die Ableitung zu berechnen.
Da dieses Verfahren für diese Arbeit sehr relevant ist, wird auf die genaue Umsetzung in Kapitel \ref{sec:ketten_regel} noch einmal genauer eingegangen.

Die vierte und letzte Möglichkeit, die Automatische-Differenzierung umzusetzen, ist den Source-Code direkt zu manipulieren.
Konzeptionell ist dieses Verfahren sehr ähnlich zu der zuvor vorgestellten Methode. 
Der große Unterschied ist dann bei dieser Methode der Code der Funktion so umgeschrieben wird, 
dass diese sowohl ihre eigentliche Ausgabe als auch ihre Ableitung selbst mit berechnet.
Dies kann auch automatisch geschehen.
So kann dafür in Julia oder C zum Beispiel ein Macro verwendet werden oder diese Funktion direkt in den Compiler mit eingebaut werden.
Diese Methoden ist Vermutliche die zur Laufzeit schnellste variante.
Hat den Nachteil, dass diese sehr schwer umzusetzen ist.
So kann zu Beispiel meistens keine if-Statements oder andere Kontroll-Fluss-Methoden innerhalb der Funktion verwendet werden. 
Diese führt dazu, dass diese Variante nur selten zum Einsatz kommt.

\subsection{Automatische Differenzierung über die Ketten Regel} \label{sec:ketten_regel}

Soll die Ableitung einer Funktion berechnet werden, 
kann die Annahme gemacht werden, 
dass diese Funktion aus primitiven Operationen aufgebaut ist, dessen Ableitung bereits bekannt ist.
So ist die folgende Funktion aus den primitiven Operationen $+$, $\cdot$ und $\sin$ aufgebaut:

$$
f(x_1, x_2, a) = (x_1 + x_2) \cdot a \cdot \sin(x_1 + x_2) = y_2
$$

Ein Programm, dass diese Funktion berechnet, könnte zum Beispiel wie folgt aussehen:

\begin{lstlisting}
    funktion f1 (x1, x2, a)
        y0 = x1 + x2
        y1 = a * sin(y0)
        y2 = y_0 * y_1
        
    	return y2
    end
\end{lstlisting}

Nun kann die Ableitung von $\frac{\partial y_2}{\partial x_1}$ wie folgt berechnet werden:

$$
	\frac{\partial y_2}{\partial x_1} = \frac{\partial (y_0 \cdot y_1)}{\partial x_1} = \frac{\partial  y_1}{\partial x_1} y_0  + \frac{\partial y_0 }{\partial x_1} y_1
$$

$$
	\frac{\partial y_1}{ \partial x_1 } = a \cdot \sin(y_0) \cdot \frac{\partial y_0}{\partial x_1}
$$

$$
	\frac{\partial y_0}{\partial x_1} = \frac{\partial (x_1 + x_2)}{\partial x_1} = 1 + 0
$$

Für $\frac{\partial y_2}{\partial x_2}$ ist das Verfahren analog. 
Nun kann die Funktion $f$ so angepasst werden, dass sie nicht nur den Wert $y_2$ berechnet sonder auch $y'_2$.

\begin{lstlisting}
    funktion f2 (x1, x2, x'1, x'2, a)
        y0 = x1 + x2
        y'0 = x'1 + x'2

        y1 = a * sin(y0)
        y'1 = a * cos(y0) * y'0

		y2 = y0 * y1
        y'2 = y'0 * y1 + y0 * y'1
        
        return (y2, y'2)
    end
\end{lstlisting}

Welche Partielle-Ableitung von $f2$ berechnet wird, hängt davon ab, wie die Argumente $x'_1$ und $x'_2$ gesetzt werden.
Wird $x'_1 = 1$ gesetzt und $x_2' = 0$ dann führt dies dazu, dass $\frac{\partial y_2}{\partial x_1}$ berechnet wird.
Um die Ableitung nach $x_2$ zu berechnen, werden die Werte einfach vertauscht.
Diese Verfahren liefert das gewünschte Ergebnis, ist aber noch lange nicht automatisch.
Es ist zwar möglich, diesen Code vom Compiler oder durch einen Macro generieren zu lassen.
In den meisten fällen wird ausgenutzt, dass es in einer 
Programmiersprachen möglich ist, Operationen zu überladen.
Das heißt, die gleiche Funktion verhält sich abhängig vom Datentype anders.
Konkret bedeutet das die Funktion $f$, wie folgt aufgerufen wird.

\begin{lstlisting}
	x1 = (v1, p1)
    x2 = (v2, p2)
    
    f1(x1, x2, 2)
\end{lstlisting}

Es fällt auf das nun $x_1$ und $x_2$ nicht mehr Fließkomma zahlen sind, sondern Tupel sind.
Damit am Ende $f1$ das gleiche Ergebnis ausgibt, wie $f2$ müssen alle in der Funktion vorkommenden Operationen überladen werden.

\begin{lstlisting}
	funktion *(a::Tupel, b::Tupel)
    	return (a[1] * b[1], a[1] * b[2] + a[2] * b[1]) 
    end
    
    funktion +(a::Tupel, b:Tuple)
    	return (a[1] + b[1], a[2] + b[2])
    end
    
    funktion sin(a:Tuple)
    	return (sin(a[1]), cos(a[1]) * a[2] ) 
    end
\end{lstlisting}

In der Realität werden für diesen Zweck keine Tupel verwendet, sondern eine \textit{Dual}-Zahl definiert.
Dessen Definition lautet wie folgt:

\begin{equation}
	d = a + b \epsilon \text{ mit } \epsilon^2 = 0
\end{equation}

Wird eine skalare Funktion nun auf die \textit{Dual}-Zahl angewendet, dann verhält sich diese wie folgt:

\begin{equation}
	f( a + b \epsilon) = f(a) + b f'(a) \epsilon
\end{equation}

Eine allgemeinere Version der \textit{Dual}-Zahl für höher dimensionale Probleme wird auch in Kapitel \ref{sec:dual_zahlen} diskutiert.
Die beschriebene Methode zum Automatischen-Differenzieren wird oft auch als Vorwärtsmodus-Differenzierung bezeichnet.
Erwähnenswert ist auch die Rückwertsmodus-Differenzierung.
Diese funktioniert sehr ähnlich zu Vorwärtsmodus-Differenzierung.
Mit dem Unterschied, dass die zwischen Ergebnis $y_0, ... , y_n$ zuerst berechnet und zwischen gespeichert werden.
Dann wird auf Grundlage der zwischen gespeicherten Werte die Ableitung berechnet.
Das heißt, anstatt die Ableitung in jeden Berechnungsschritt mit zuziehen, wird diese erst am Ende für alle freien Variablen berechnet.
Dies hat den großen Vorteil, dass die Laufzeit des Algorithmus von der Berechnungszeit der Funktion $f$ abhängt und nicht von der Anzahl der Parameter.
Der Nachteil dieser Methoden ist, dass dafür wesentlich mehr Speicherplatz verbraucht wird.
Welcher dieser beiden Verfahren besser geeignet ist, hängt stark vom Problem ab.
Bei einem neuronalen Netz gibt es eine große Anzahl an freien Parametern, was die Rückwärtsmodus-Differenzierung bevorzugt.
Die meisten Differenzialgleichungen haben in der Regel nur 4 freie Variablen.
Die erste frei Variable ist meistens die Zeit und dann kommen noch die 3 Raumdimensionen hinzu.
Diese führt dazu, dass die Vorwärzmodus-Differenzierung bei Differenzialgleichungen meist besser geeignet ist.
Dies sollte jedoch mit Vorsicht genossen werden, da es immer Ausnahmen gibt.
