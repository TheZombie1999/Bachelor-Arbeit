% https://en.wikipedia.org/wiki/Automatic_differentiation
% https://www.youtube.com/watch?v=wG_nF1awSSY

\section{Automatisches Differenzieren} \label{sec:auto_diffentiation}

Beim automatischen Differenzieren \cite[Kapitel~2]{autodiff} geht es darum, die Ableitung einer Funktion automatisch zu berechnen.
Das Besondere dabei ist, dass diese meistens als Computerprogramm gegeben ist.
Automatisches Differenzieren findet Anwendung beim Trainieren von neuronalen Netzen 
und dem Lösen von nicht-linearen Gleichungssystemen durch das Newton-Verfahren.
Der zweite Punkt ist für diese Arbeit besonders relevant, da 
dieses benötigt wird, um die Runge-Kutta-Verfahren, die in Kapitel \ref{sec:runge_kutta} vorgestellt wurden, zu berechnen.
Die mathematische Definition der Funktion lautet wie folgt:

$$
f: \mathbb{R}^n \rightarrow \mathbb{R}^m, x \mapsto y
$$

Gesucht werden nun die partiellen Ableitungen bzw. die Jacobo-Matrix von $f$:

$$
\frac{\partial f}{\partial x} = \left[ \frac{\partial y_i}{\partial x_i} \right]_{i=1..m, j=1..n} 
$$

Die Jacobi-Matrix wird berechnet, wenn alle partiellen Ableitungen einer Funktion benötigt werden.
Wird nur eine konkrete partielle Ableitung benötigt, kann diese ebenfalls mit Autodifferenzierung berechnet werden.

\subsection{Implementierungen} \label{sec:implementation}

Es gibt grundsätzlich vier Wege, automatische Differenzierung umzusetzen.
Die einfachste Variante approximiert die Limit-Definition \cite[Kapitel~2.8]{fund_cal}, in dem sie $h$ auf einen kleinen Wert
$h_{min}$ setzt.


$$
\frac{\partial f_k}{\partial x} = \lim_{h \to 0} \frac{f_k(x+ h) - f_k(x)}{h} \approx \frac{f_k(x + h_{min}) - f_k(x)}{h_{min}} 
$$

Diese Methode ist sehr leicht zu implementieren, allerdings ist sie sehr ungenau und instabil.
Eine exakte Lösung ergibt sich mit der Verwendung eines Computer-Algebra-Systems wie zum Beispiel Sympy \cite{sympy}.
Dieses Verfahren operiert auf der symbolischen Repräsentation der Funktion.
Es verwendet die unterschiedlichen Ableitungsregeln, um eine exakte Lösung zu berechnen.
Dieses Verfahren hat das Problem, dass häufig verwendete Ableitungsregeln wie zum Beispiel die Produktregel
mit jeder Anwendung die Ergebnisgröße verdoppelt, wodurch es zu einem exponentiellen Wachstum 
der symbolischen Repräsentation kommt.
Dies macht dieses Verfahren unbrauchbar, um die Ableitung eines neuronalen Netzes zu berechnen.
Die dritte Methode nutzt aus, dass Operationen in einer Programmiersprache überladen werden können.
Es wird dazu angenommen, dass die Funktion, dessen Ableitung berechnet werden soll, aus primitiven Operationen aufgebaut ist, dessen Ableitung bekannt ist.
Nun kann die Kettenregel ausgenutzt werden, um Schritt für Schritt die Ableitung zu berechnen.
Da dieses Verfahren für diese Arbeit sehr relevant ist, wird auf die Umsetzung in Kapitel \ref{sec:ketten_regel} noch einmal eingegangen.

Die vierte und letzte Möglichkeit, die Automatischedifferenzierung umzusetzen, ist den Source-Code direkt zu manipulieren.
Konzeptionell ist dieses Verfahren sehr ähnlich zu der zuvor vorgestellten Methode. 
Der große Unterschied bei dieser Methode ist, dass der Code der Funktion so umgeschrieben wird, 
dass diese sowohl ihre eigentliche Ausgabe als auch ihre Ableitung selbst mit berechnet.
Dies kann auch automatisch geschehen.
So kann dafür in Julia oder C zum Beispiel ein Makro verwendet werden oder diese Funktion direkt in den Compiler mit eingebaut werden.
Diese Methoden sind die zur Laufzeit schnellste Variante.
Der Nachteil ist, dass dies sehr schwer umzusetzen ist.
So können zu Beispiel meist keine \textit{if-Statements} oder andere Kontroll-Fluss-Methoden innerhalb der Funktion verwendet werden. 
Diese führt dazu, dass diese Variante nur selten zum Einsatz kommt.

\subsection{Automatische Differenzierung über die Kettenregel} \label{sec:ketten_regel}

Soll die Ableitung einer Funktion berechnet werden, 
kann die Annahme gemacht werden, 
dass diese Funktion aus primitiven Operatoren aufgebaut ist, dessen Ableitungen bereits bekannt sind.
Die Funktion aus der Quelle \cite{autodiff_example} ist aus den primitiven Operatoren $+$, $\cdot$ und $\sin$ aufgebaut und illustriert die allgemeine Vorgehensweise.

$$
f(x_1, x_2, a) = (x_1 + x_2) \cdot a \cdot \sin(x_1 + x_2) = y_2
$$

Ein Programm, das diese Funktion berechnet, könnte zum Beispiel wie folgt aussehen:

\begin{lstlisting}[language=Julia]
    function f1 (x1, x2, a)
        y0 = x1 + x2
        y1 = a * sin(y0)
        y2 = y_0 * y_1
        
    	return y2
    end
\end{lstlisting}

Nun kann die Ableitung von $\frac{\partial y_2}{\partial x_1}$ wie folgt berechnet werden:

$$
	\frac{\partial y_2}{\partial x_1} = \frac{\partial (y_0 \cdot y_1)}{\partial x_1} = \frac{\partial  y_1}{\partial x_1} y_0  + \frac{\partial y_0 }{\partial x_1} y_1
$$

$$
	\frac{\partial y_1}{ \partial x_1 } = a \cdot \sin(y_0) \cdot \frac{\partial y_0}{\partial x_1}
$$

$$
	\frac{\partial y_0}{\partial x_1} = \frac{\partial (x_1 + x_2)}{\partial x_1} = 1 + 0
$$

Für $\frac{\partial y_2}{\partial x_2}$ ist das Verfahren analog. 
Nun kann die Funktion $f$ so angepasst werden, dass sie nicht nur den Wert $y_2$ berechnet, sondern auch $y'_2$.

\begin{lstlisting}[language=Julia]
    function f2 (x1, x2, x'1, x'2, a)
        y0 = x1 + x2
        y'0 = x'1 + x'2

        y1 = a * sin(y0)
        y'1 = a * cos(y0) * y'0

		y2 = y0 * y1
        y'2 = y'0 * y1 + y0 * y'1
        
        return (y2, y'2)
    end
\end{lstlisting}

Welche Partielle-Ableitung von $f2$ berechnet wird, hängt davon ab, wie die Argumente $x'_1$ und $x'_2$ gesetzt werden.
Wird $x'_1 = 1$ gesetzt und $x_2' = 0$ dann führt dies dazu, dass $\frac{\partial y_2}{\partial x_1}$ berechnet wird.
Um die Ableitung nach $x_2$ zu berechnen, werden die Werte einfach vertauscht.
Diese Verfahren liefert das gewünschte Ergebnis, ist aber nicht automatisch.
Es ist möglich diesen Code vom Compiler oder durch einen Makro generieren zu lassen.
In den meisten Fällen wird ausgenutzt, dass es in einer 
Programmiersprachen möglich ist, Operatoren zu überladen.
Das heißt, die gleiche Funktion verhält sich abhängig vom Datentype anders.
Für die Funktion $f1$ würde das zum Beispiel wie folgt aussehen.

\begin{lstlisting}[language=Julia]
	x1 = (v1, p1)
    x2 = (v2, p2)
    
    f1(x1, x2, 2)
\end{lstlisting}

$x_1$ und $x_2$ sind hier keine Fließkommazahlen, sondern Tupel.
Damit am Ende $f1$ das gleiche Ergebnis ausgibt, wie $f2$ müssen alle in der Funktion vorkommenden Operationen überladen werden.

\begin{lstlisting}[language=Julia]
	function *(a::Tupel, b::Tupel)
    	return (a[1] * b[1], a[1] * b[2] + a[2] * b[1]) 
    end
    
    function +(a::Tupel, b:Tuple)
    	return (a[1] + b[1], a[2] + b[2])
    end
    
    function sin(a:Tuple)
    	return (sin(a[1]), cos(a[1]) * a[2] ) 
    end
\end{lstlisting}

In der Realität werden für diesen Zweck keine Tupel verwendet, sondern eine \textit{Dual}-Zahl definiert.
Dessen Definition lautet wie folgt:

\begin{equation}
	d = a + b \epsilon \text{ mit } \epsilon^2 = 0
\end{equation}

Wird eine skalare Funktion nun auf die \textit{Dual}-Zahl angewendet, dann verhält sich diese wie folgt:

\begin{equation}
	f( a + b \epsilon) = f(a) + b f'(a) \epsilon
\end{equation}

Eine allgemeinere Version der \textit{Dual}-Zahl für höher dimensionale Probleme wird auch in Kapitel \ref{sec:dual_zahlen} diskutiert.
Die beschriebene Methode zum Automatischen-Differenzieren wird oft auch als Vorwärtsmodus-Differenzierung bezeichnet.
Erwähnenswert ist auch die Rückwärtsmodus-Differenzierung.
Diese funktioniert sehr ähnlich zu Vorwärtsmodus-Differenzierung
mit dem Unterschied, dass die zwischen Ergebnis $y_0, ... , y_n$ zuerst berechnet und zwischengespeichert werden.
Dann wird auf Grundlage dieser die Ableitung berechnet.
Das heißt, anstatt die Ableitung in jeden Berechnungsschritt mit zuziehen, wird diese erst am Ende für alle freien Variablen berechnet.
Dies hat den großen Vorteil, dass die Laufzeit des Algorithmus von der Berechnungszeit der Funktion $f$ abhängt und nicht von der Anzahl der Parameter.
Der Nachteil dieser Methoden ist, dass dafür wesentlich mehr Speicherplatz verbraucht wird.
Welcher dieser beiden Verfahren besser geeignet ist, hängt stark vom Problem ab.
Bei einem neuronalen Netz gibt es eine große Anzahl an freien Parametern, was die Rückwärtsmodus-Differenzierung bevorzugt.
Die meisten Differenzialgleichungen bestehen aus drei Raum- und einer Zeit-Dimension.
Dies führt dazu, dass die Vorwärtsmodus-Differenzierung bei Differenzialgleichungen meist besser geeignet ist.
Dies ist jedoch nicht immer der Fall.
