
\section{Anfangswertprobleme} \label{sec:anfangswert_probleme}
% Numerical Integraion book
% https://books.google.de/books?hl=de{sec:explizites_euler_verfahren}&lr=&id=gGCKdqka0HAC&oi=fnd&pg=PP1&dq=numerical+integration&ots=NDzzuAvLhM&sig=3_fDwVwrMX_kgecuXDtsKIZ6pgE&redir_esc=y#v=onepage&q&f=false
% https://en.wikipedia.org/wiki/Numerical_integration#:~:text=In%20analysis%2C%20numerical%20integration%20comprises,numerical%20solution%20of%20differential%20equations.

Anfangswertprobleme treten in vielen Naturwissenschaften auf, denn sie erlauben es, 
den zeitlichen Verlauf eines Systems zu modellieren.
Um dies umzusetzen, wird eine Differenzialgleichung benötigt, welche die Veränderung des Systems beschreibt.
Diese ist jedoch nutzlos, falls nicht der initiale Zustand des Systems bekannt ist.
Für diese Arbeit sind vor allem Anfangswertprobleme erster Ordnung interessant.
Deren Definition setzt sich aus der Differenzialgleichung und der Anfangsbedingung zusammen:

$$
\frac{dy}{dt} = f(t, y(t)) \text{ mit der Anfangsbedingung } y(t_0) = y_0
$$

Ein bekanntes Beispiel in der Physik ist die vereinfachte Beschleunigung eines Autos.
Es wird dabei angenommen, dass keine Reibung vorliegt und der Motor mit einer konstanten Kraft auf dieses wirkt.
Das daraus entstehende Anfangswertproblem lautet wie folgt:

$$
a = \frac{dv}{dt} = k \text{ wobei k const.}
$$

Die Geschwindigkeit am Anfang ist gleich null, demnach gilt  $v(0) = 0$.
Dieses System hat eine einfache, exakte Lösung:
$$
v(t_1) = v_0 + \int_{t_0}^{t_1} k dt = v_0 + k \cdot (t_1 - t_0) 
$$

Im Allgemeinen ist es aber nicht immer möglich, 
ein Anfangswertproblem analytisch zu lösen. 
In den nächsten Kapiteln werden mehrere Verfahren vorgestellt, diese numerisch zu berechnen.
Ein Beispiel für ein solches Problem wird in Kapitel \ref{sec:simulationen} vorgestellt.

\section{Numerisches Lösen von Anfangswertproblemen} \label{sec:numerisches_lösen_von_anfangswert_problemen}

\subsection{Explizites Euler-Verfahren} \label{sec:explizites_euler_verfahren}

% https://www.biancahoegel.de/mathe/verfahr/euler-verfahren_explizit.html

Das einfachste Verfahren, um Anfangswertprobleme zu lösen, ist das explizite Euler-Verfahren \cite[Kapitel~II.1]{ode1}.
Dieses approximiert verschiedene Werte des Anfangswertproblems 
im Abstand einer konstanten Schrittweite h.
Die Definition der zu approximierenden Zeitpunkte lautet:

$$
t_k = t_0 + kh \text{, mit } k = 0, 1, 2, ...
$$

Die approximierten Werte von $y$ werden wie folgt berechnet:

$$
y_{k + 1} = y_{k} + h \cdot f(t_k, y_k)
$$

Das Euler-Verfahren basiert auf folgender Approximation des zu berechnenden Integrals:

$$
\int_{t_k}^{t_{k+1}} f(s, y_k(s)) ds \approx h f(t_k, y_k)
$$

Dies ist wichtig, da alle Verfahren zum numerischen Lösen von Anfangswertproblemen sich darin unterscheiden, wie sie diese Approximation berechnen.
Das explizite Euler-Verfahren mit einer \textit{NeuralODE} zu berechnen
ist leicht, da $y_k$, h und $f(t_k, y_k)$ bekannt sind.


\subsection{Implizites Euler-Verfahren}

Das implizite Euler-Verfahren  \cite[Kapitel~II.7]{ode1} ist sehr ähnlich zu der expliziten Variante.
Der entscheidende Unterschied ist, dass nicht die Steigung abhängig vom aktuellen 
Zeitpunkt und y-Wert verwendet wird, sondern vom nächsten Zeitschritt.
Die Definition lautet deshalb wie folgt:

$$
y_{k + 1} = y_k + h \cdot f(t_{k + 1}, y_{k + 1})
$$

Die restliche Definition ist analog zum expliziten Euler-Verfahren aus Kapitel \ref{sec:explizites_euler_verfahren}.
Das Besondere dabei ist, dass $y_{k + 1}$ nicht bekannt ist.
Demnach enthält $f(t_{k + 1}, y_{k + 1})$ auch eine Unbekannte.
Um das Gleichungssystem zu lösen, muss nach $y_{k + 1}$ aufgelöst werden.
Im Allgemeinen kann allerdings nicht davon ausgegangen werden, dass dies immer effizient möglich ist.
Deshalb wird für das Lösen des Gleichungssystems das Newton-Verfahren \cite[Kapitel~5.5]{intorduction_to_numerical_analysis} verwendet.
Das Sekanten-Verfahren \cite[Kaptiel~18.2]{HankeBourgeois.2002} könnte ebenfalls genutzt werden, ist aber aufgrund seines hohen numerischen 
Fehlers meist sehr unattraktiv.
Um das Newton-Verfahren anwenden zu können, muss das Problem wie folgt umgeformt werden:

$$
0 = (y_k - y_{k + 1})  + h \cdot f(t_{k + 1}, y_{k + 1})
$$

Nun wird die Funktion $g(x)$ wie folgt definiert:
$$
x = y_{k + 1}
$$
$$
g(x) := (y_k - x)  + h \cdot f(t_{k + 1}, x)
$$
$$
g'(x) = -1 + h \cdot \frac{\partial f(t_{k+1}, x)}{\partial x}
$$

Nun kann dessen Nullstelle über das Newton-Verfahren bestimmt werden:

$$
x_{n+1} = x_{n} - \frac{g(x_n)}{g'(x_n)}
$$

Dabei ist es wichtig, $x_{0}$ auf einen sinnvollen Wert zu setzen, ein guter Kandidat ist zum Beispiel
$y_{k}$, da dieser bekannt und nahe am gesuchten Wert liegt.
Damit das implizite Euler-Verfahren berechnet werden kann, wird die Ableitung von $f(t_{k + 1}, y_{k + 1})$ benötigt.

Werden \textit{MeshGraphNets} verwendet,
dann ist die Funktion f ein neuronales Netz.
Dessen Ableitung mit einem Algorithmus zur automatischen Differenzierung berechnet wird.
Dessen Funktionsweise wird in Kapitel \ref{sec:auto_diffentiation} beschrieben.

\subsection{Die Runge-Kutta-Verfahren} \label{sec:runge_kutta}

Die Runge-Kutta-Verfahren \cite[Kapitel~II.1]{ode1} sind eine Verallgemeinerung des
bereits vorgestellten impliziten/expliziten Euler-Verfahrens.
Die Runge-Kutta-Verfahren verwenden folgende Approximation:

$$
\int_{t_k}^{t_{k+1}} f(t_k, y_k(s)) ds \approx h \sum_{j=1}^{s} b_j k_j
$$
wobei $k_j$ wie folgt definiert ist:
$$
k_j = f(t_n + h c_j, y_n + h \sum_{l=1}^{s}a_{jl}k_{l}) , j \in \{1, ..., s\}
$$
% https://www.youtube.com/watch?v=6cqn8cnYRUg&list=PLgPpaTsP_3DqH0RNVsSOiohhGQz_7vf_R&index=7

Die Parameter $c_j, b_j, a_{jl}$ sind dabei charakteristisch,
für das jeweilige Verfahren.
Falls gilt $a_{jl} = 0$ für alle Werte beidem gilt $l \ge j$, dann ist das Verfahren explizit.
Die Idee hinter dem Runge-Kutta-Verfahren ist, dass der Bereich zwischen dem aktuellen und dem nächsten Zeitschritt in mehrere Teile zerlegt wird.  
An jedem dieser Punkte wird die Steigung $k_i$ bestimmt.
Mit den Parametern $b_j$ wird, dann deren gewichtete Summe berechnet.
Der Parameter $c_j$ ist dafür verantwortlich, wie die Zeit unterteilt wird und $b_j$ wie der Bereich zwischen $y_i$ und $y_{i+1}$ gegliedert wird.
Um die Runge-Kutta-Verfahren lösen zu können, wird das Newton-Verfahren für ein 
System von Gleichungen verwendet.
Diese ist eine Verallgemeinerung des zuvor gezeigten Newton-Verfahrens.
Die Grundvoraussetzung für dessen Berechnung ist jedoch die gleiche.
Mit dem einzigen Unterschied, dass nun nicht nur die Ableitung nach $x$ benötigt wird, sondern alle Partiellenableitungen von $f$.

\subsubsection{Das Newtonverfahren für mehrdimensionale Gleichungssysteme}

Um die Runge-Kutta-Verfahren lösen zu können, 
wird eine angepasste Variante des Newton-Verfahrens \cite[Kaptitel~5.1]{intorduction_to_numerical_analysis}, 
verwendet, welches wie folgt lautet:

$$
x_{n+1} = x_n - (J(x_n))^{-1} f(x_n)
$$

$x_n$ ist ein Vektor aus $\mathbb{R}^n$. 
Außerdem ist $f$ ein Vektor von Funktionen $[f_1, ..., f_n]$.
$J$ steht dabei für die Jacobi-Matrix, welche wie folgt definiert ist:
$$
J(x) = \left( \frac{\partial f_i}{\partial x_j}(x) \right)_{i, j} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \dots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \dots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \dots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}
$$

Da es sehr aufwendig ist, das Inverse einer Matrix zu berechnen, wird das Newton-Verfahren zu einem linearen Gleichungssystem mit einer unbekannten $\Delta x_n$ um formuliert.
$$
J(x_n) \Delta x_n = -f(x_n)
$$

Der nächste Schritt wird dann wie folgt berechnet:
$$
x_{n+1} = x_n + \Delta x_n
$$

Die Korrektheit dieses Verfahrens ist leicht zusehen, wenn $\Delta x_n $
wie folgt definiert wird:
$$
\Delta x_n := -(J(x_n))^{-1} f(x_n)
$$

Da $f(x_n)$ und $J(x_n)$ sich berechnen lassen, 
erlaubt diese Darstellung das Newton-Verfahren auf mehrdimensionale Daten anzuwenden. 


\subsection{Mehr-Schritt-Methoden (Multistep Methods)} \label{sec:multi_step}

Mehr-Schritt-Methoden \cite[Kapitel~III.1]{ode1} verbessern die Approximation der analytischen Lösung, indem sie die
zuvor berechneten Punkte mit in die Lösung einbeziehen.
Im Allgemeinen sind die Mehr-Schritt-Methoden wie folgt definiert:
$$
\sum_{j= 0}^{s} a_j y_{n+j} = h \sum_{j=0}^s b_j f(t_{n+j}, y_{n+j})
$$

Wobei gilt $a_s = 1$.  Die Parameter $a_0 ... a_{s-1}$ und $b_0 ... b_{s}$ sind entscheidend dafür, welche Methode verwendet wird.
Durch den Parameter $b_s$ wird bestimmt, ob das Verfahren explizit oder
implizit ist.
Um die Parameter $a_i$ und $b_j$ zu bestimmen, wird das Lagrange-Polynom \cite[Kapitel~3.1.2]{numerisch_langrange} verwendet.
Dies ist definiert, als das kleinste Polynom, dass eine Menge von Punkten interpoliert.

$$
l_j(x) = \prod_{x \leq m \leq k \\ m \neq j} \frac{x - x_m}{x_j - x_m}
$$

$$
L(x) = \sum_{j = 0}^{k} y_j l_j(x)
$$

Wobei die Menge der zu interpolierenden Punkte, wie folgt festgelegt ist:

$$
\{(x_0, y_0), ..., (x_k, y_k)\}
$$

Die Intuition hinter der Definition des Lagrange-Polynoms ist, 
dass das Polynom $l_j(x)$ eine Null-Stelle an jedem Punkt $x_i$ hat, außer für den Punkt bei dem gilt $i = j$. 
An dieser Stelle hat das Polynom $l_j(x)$ den Wert 1, wegen des Quotienten $\frac{1}{x_j - x_m}$.
Wird nun $l_j(x)$ mit $y_j$ multipliziert, ergibt sich ein Polynom mit dem richtigen y-Wert an der Stelle $x_j$ und den Wert null an jedem anderen Punkt $x_i$.
Werden nun diese Polynome für jeden Punkt aufaddiert,
ergibt sich das Langrange-Polynom, dass an jeder Stelle $x_i$ garantiert den richtigen Wert $y_i$ hat.


\subsubsection{Adam-Bashforth-Verfahren} \label{sec:adam-bashforth}

Das Adam-Bashforth-Verfahren ist eine explizite Mehr-Schritt-Methode, 
die das Lagrange-Polynom verwendet, 
um die Steigung für den nächsten Zeitschritt abzuschätzen. 
Dafür interpoliert das Verfahren über die Steigung der letzten $s$ Schritte.
Konkret bedeutet dies, dass das Lagrange-Polynom $p$ auf die Punkte $\{(t_{n}, f(t_{n}, y_{n}), ..., (t_{n + s - 1}, f(t_{n + s - 1}, y_{n + s - 1}))\}$ angewendet wird.
Der nächste Zeitschritt kann, dann wie folgt berechnet werden:

$$
y_{n + s} = y_{n + s -1} + \int_{t_{n+s-1}}^{t_{n + s}} p(t) dt
$$

Für die Koeffizienten gilt, dass $a_{s-1} = -1$ und $a_{s-2} ... a_{0} = 0$. 
Da dieses Verfahren explizit ist, gilt für $b_s = 0$. Die restlichen Koeffizienten $b_{s-1}, ... b_{0}$
können, dann wie folgt berechnet werden:

$$
b_{s - j - 1} = \frac{(-1)^j}{j!(s- j -1 )!} \int_0^1 \prod_{i = 0 \\ j \neq j}^{s-1} (u + i) du \text{ mit } j = 0, ...,  s-1 
$$

\subsubsection{Adam-Moulton-Verfahren} \label{adam-moulton}

Das Adam-Moulton-Verfahren ist ähnlich zum
Adam-Bashforth-Verfahren, mit dem einzigen Unterschied, dass dieses implizit ist.
Das heißt, dass das Lagrange-Polynom auf die Punkte $\{(t_{n}, f(t_{n})), ... (t_{n+s}, f(t_{n+s}))\} $ angewendet wird.
Daraus folgt für die Koeffizienten $b_{0}, ..., b_{s}$ folgende Definition:

$$
b_{s - j} = \frac{(-1)^j}{j!(s - j)!} \int_0^{1} \prod_{i = 0 \\ i \neq j}^{s} (u + i - 1) du, \text{ mit j = 0, ..., s}
$$

Die Koeffizienten $a_0, ..., a_s$ sind analog zum Adam-Bashforth-Verfahren.


\subsubsection{BDF-Verfahren} \label{sec:bdf}

Die BDF-Verfahren (englisch Backward Differentiation Formulas) interpolieren im Vergleich zu den zwei zuvor genannten Verfahren nicht 
die Steigungen der einzelnen Punkte, sondern dessen y-Werte.
Das heißt, das Lagrange-Polynom $p$ wird auf die Punkte $\{ (t_n, y_n), ..., (t_{n + s}, y_{n + s}) \}$ angewendet.
Um die Koeffizienten $a_0, ..., a_k$ bestimmen zu können, muss außerdem noch folgende Bedingung gelten:

$$
p'(t_{n + s}) = f(t_{n + s}, y_{n + s})
$$

Um nun die Koeffizienten zu bestimmen, wird zunächst die erste Ableitung des Langrange-Polynoms benötigt:

$$
p'(x) := \sum_{j = 0}^{k} y_j l'_j(x)
$$
$$
l'_j(x) := \sum_{i = 0 \\ i \neq j}^{k} \left[ \frac{1}{x_j - x_i} \prod_{m = 0 \\ m \neq (i, j)}^k \frac{x - x_m}{x_j - x_m} \right]
$$

Daraus folgt nun der Wert für die Parameter $a_0, ... ,a_s$:

$$
a_j = l'_j(x_{n + k}) \text{, mit j = 0, ..., s}
$$

Aus dieser Bedingung folgt außerdem, das $b_0, ...,b_{s - 1} = 0$ sind und $b_s = 1$.
Dadurch ergibt sich ein nicht-lineares Gleichungssystem, mit dem implizit gegebenen Wert $y_{k + s}$.
Dies kann nun wieder mit dem Newton-Verfahren gelöst werden.
Eine wichtige Anmerkung ist, 
dass für $s > 6$ die BDF-Formeln nicht mehr angewendet werden sollten, 
da das Verfahren ab diesem Grad nicht mehr null-stabil ist.
