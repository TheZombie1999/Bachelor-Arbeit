
\section{Anfangswert Probleme}
% Numerical Integraion book
% https://books.google.de/books?hl=de&lr=&id=gGCKdqka0HAC&oi=fnd&pg=PP1&dq=numerical+integration&ots=NDzzuAvLhM&sig=3_fDwVwrMX_kgecuXDtsKIZ6pgE&redir_esc=y#v=onepage&q&f=false

% https://en.wikipedia.org/wiki/Numerical_integration#:~:text=In%20analysis%2C%20numerical%20integration%20comprises,numerical%20solution%20of%20differential%20equations.

% Anfangswert Probleme


Ein Anfangswertproblem 1. Ordnung ist, ein Gleichungssystem das wie folgt definiert ist:
$$
\frac{dy}{dt} = f(t, y(t)) \text{ mit der anfangs bedingung } y(t_0) = y_0
$$

Anfangs wert Probleme spielen eine Große Rolle in der Physik.

Ein bekanntes beispiel ist die beschleunigtes auto.

Beschleunigt ein auto mit konstanter beschleunigung so lautet, dass daraus entstehende anfangswertproblem:


$$
a = \frac{dv}{dt} = k \text{ wobei k const.}
$$

die Geschwindichkeit am anfang ist 0 daher gilt:

$$
v(0) = 0
$$

Dieses System hat eine Einfache exacte Lösung:
$$
v = v_0 + \int_{t_0}^{t_1} k dt = k \cdot (t_0 - t_1) 
$$

Im allgemeinen ist es aber nicht immer möglich ein Anfangswertproblem analytisch zu lösen, deshalb wurde verschiedene  verfahren entwickelt um anfangswert problem numerisch zu lösen.

Ein Beispiel für ein solches Anfangswert Problem wird in \ref{} Vorgestellt.


\section{Numerisches Lösen von Anfangswert problemen}

\subsection{Explizite Euler Verfahren}

% https://www.biancahoegel.de/mathe/verfahr/euler-verfahren_explizit.html

Das einfachste verfahren zum Lösen von Anfangswert problemen ist das explizite Euler verfahren.

Das explizit euler verfahren approximiert verschiedene werte des Anfangswert problem 
im abstand einer konstanten schrittweite h .

Die approximierten zeitpunkte sind wie folgt definiert.
$$
t_k = t_0 + kh \text{, mit } k = 0, 1, 2, ...
$$

Die approximierten werte von y werden dann wie folgt berechntet:

$$
y_{k + 1} = y_{k} + h \cdot f(t_k, y_k)
$$

Das Euler-Verfahren verwendet folgende approxmation:

$$
\int_{t_k}^{t_{k+1}} f(t_k, y_k(s)) ds \approx h f(t_k, y_k)
$$

Dies ist wichtig, da alle verfahren zum numerischen lösen von anfangswert problemen, sich hauptsächlich in dieser approximation unterscheiden.

Das Expliziten Euler-verfahren mit einer neural ode zu berechnen
ist sehr leicht, da $y_k$, h und $f(t_k, y_k)$ bekannt sind.

Das nächste verfahren das vorgestellt wird verändert die art und weiße der Approximation, was dazu führt das die berechnung nicht mehr so direkt durchgeführt werden kann.



\subsection{Implizite Euler Verfahren}

Das implizite Euler verfahren ist sehr ähnlich zu der expliziten variante.

Der entscheidende unterschied ist, dass das implizite verfahren nicht die steigung abhänig von aktuellen 
zeitpunkt und y-wert verwendet, sondern vom nächsten zeitpunkt.

Die Definition des lautet deshalb wie folgt.

$$
y_{k + 1} = y_k + h \cdot f(t_{k + 1}, y_{k + 1})
$$

Der rest der definition ist analog zum expliziten euler verfahren aus Kaptiel \ref{}.

Das besondere dabei ist, dass $y_{k + 1}$ nicht bekannt ist.
Demnach enthält $f(t_{k + 1}, y_{k + 1})$ auch eine unbekannte.

Es liegt also ein gleichungs system mit einer unbekanntern welches gelößt werden muss.

Dazu wird meist das Newton verfahren verwendet.

Dazu muss die gleichung zunächst umgeformt werden.

Um dies Anwenden zu können muss dies erst umgefort werden.

$$
0 = (y_k - y_{k + 1})  + h \cdot f(t_{k + 1}, y_{k + 1})
$$

Nun wird die Funktion $g(x)$ wie folgt definiert:

$$
g(x) := (y_k - y_{k + 1})  + h \cdot f(t_{k + 1}, y_{k + 1}) = (y_k - x)  + h \cdot f(t_{k + 1}, x)
$$
$$
g'(x) = -1 + h \cdot \frac{\partial f(t_{k+1}, x)}{\partial x}
$$

Nun kann dessen Nullstelle über das Newton verfahren wie folgt bestimmt werden.

$$
x_{n+1} = x_{n} - \frac{g(x_n)}{g'(x_n)}
$$

Dabei ist des wichtig $x_{0}$ auf eine Sinnvollen wert zu setzen ein guter kandidat ist zum beispiel
$y_{k}$ da dieser bekannt und nahe am gesuchten wert liegt.

Wichtig ist das damit das implizit euler verfahren berechnet werden kann die ableitung von $f(t_{k + 1}, y_{k + 1})$ benötigt wird.

In userem fall handelt es sich hierbei um eine NeuralODE dessen ableitung mit einem automatischen ableitungs algorithmus berechnet wird. Wie dies funktioniert wird in Kaptitle \ref{} beschrieben.



\subsection{Die Runge Kutta verfahren}

Die Runge Kutta verfahren sind eine verallgemeinerung der
bereits vorgestellten impliziten/expliziten Euler verfahren.

Die Runge Kutta verfahren verwenden folgende approximation:

$$
\int_{t_k}^{t_{k+1}} f(t_k, y_k(s)) ds \approx h \sum_{j=1}^{s} b_j k_j
$$
wobei $k_j$ wie folgt definiert ist.
$$
k_j = f(t_n + h c_j, y_n + h \sum_{l=1}^{s}a_{jl}k_{l}) , j \in \{1, ..., s\}
$$
% https://www.youtube.com/watch?v=6cqn8cnYRUg&list=PLgPpaTsP_3DqH0RNVsSOiohhGQz_7vf_R&index=7


Um Runge Kutta Methoden lösen zu können, wird im allgemeinen die Newton's Methode für ein 
system von gleichungen verwendet werden.

Dieses unterscheidet sich von den normalen Newton verfahren.

Die grund vorraussetzung für dessen berechung ist jedoch die gleiche.

Es werden die Paritellen ableitungen von der funktion 
$f$ benötigt.

\subsubsection{Das Newtonverfahren für Mehrdimensionale Gleichungssysteme}

Um die Runge-Kutta-Methoden lösen zu können, wird eine angepasste variante des Newton verfahren, verwendet
welches wie folgt lautet:
$$
x_{n+1} = x_n - (J(x_n))^{-1} f(x_n)
$$

$x$ und $h$ sind dabei Vektoren aus $\mathbb{R}^n$. 
Ausserdem ist $f$ ein vektor von funktionen $[f_1, ..., f_n]$.
$J$ steht dabei für die Jacobi Matrix welche wie folgt definiert ist:
$$
J(x) = \left( \frac{\partial f_i}{\partial x_j}(x) \right)_{i, j} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \dots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \dots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \dots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}
$$

Da es sehr Aufwändig ist das inverse einer Matrix zu berechnen, wird das newton verfahren zu einen LineareGleichungs system mit einer unbekannten $\Delta x_n$ um formuliert.
$$
J(x_n) \Delta x_n = -f(x_n)
$$

Der nächste schritt wird dann wie folgt berechnet:
$$
x_{n+1} = x_n + \Delta x_n
$$

Die Korrektheit dieses verfahren ist leicht zusehen wenn $\Delta x_n $
wie folgt definiert wird:
$$
\Delta x_n := -(J(x_n))^{-1} f(x_n)
$$


\subsection{Mehr Schritt Methoden (Multistep Methods)}

Mehr schritt Methoden verbessern die Appoximation der Analystischen lösung, indem sie 
die information der letzten $s$ bereits berechneten Punkte mit in die lösung mit einbeziehen.

Es gibt mehre versionen dies umzusetzen.

Im allgemeinen sind multistep-methoden wie folgt definiert:
$$
\sum_{j= 0}^{s} a_j y_{n+j} = h \sum_{j=0}^s b_j f(t_{n+j}, y_{n+j})
$$
Wobei gilt $a_s= 1$.  Die Parameter $a_0 ... a_{s-1}$ und $b_0 ... b_{s}$ sind entscheident dafür welche Methode verwendet wird.

Besonders augenmerk, fällt dabei auf $b_s$ da dieser entscheidet ist ob das verfahren explizit oder implizit ist.

Um die Parameter $a_i$ und $b_j$ zu bestimmen wird das lagrage polynom verwendet.

Das lagrage polynom ist das kleinste polynom das ein Menge von Punkten interpoliert.

Dies ist wie folgt definiert:
$$
l_j(x) = \prod_{x \leq m \leq k \\ m \neq j} \frac{x - x_m}{x_j - x_m}
$$

$$
L(x) = \sum_{j = 0}^{k} y_j l_j(x)
$$

Wobei die Menge der zu interpolierenden Punkte wie folgt definiert ist:
$$
\{(x_0, y_0), ..., (x_k, y_k)\}
$$

\subsubsection{Adam-Bashforth Verfahren}

Das Adam-Bashforth Verfahren ist ein Explizites verfahren, 
dass das Lagrage-Polynom verwendet um die steigung die für den nächsten zeitschritt
verwendet wird abzuschätzen. 

Dafür interpoliert das verfahren über die steigung der letzten s schritte.

Konkeret bedeutet dies, dass das Lagrage polynom $p$ auf die punkte $\{(t_{n}, f(t_{n}, y_{n}), ..., (t_{n + s - 1}, f(t_{n + s - 1}, y_{n + s - 1})\}$ angewendet wird.

Der nächste Zeitschritt kann dann wie folgt berechnet werden:
$$
y_{n + s} = y_{n + s -1} + \int_{t_{n+s-1}}^{t_{n + s}} p(t) dt
$$
Für die koeffizentien gilt, dass $a_{s-1} = -1$ und $a_{s-2} ... a_{0} = 0$. 

Da dieses verfahren explizit ist gilt für $b_s = 0$. Die restlichen koeffizienten $b_{s-1}, ... b_{0}$
können dann wie folgt berechnet werden:

$$
b_{s - j - 1} = \frac{(-1)^j}{j!(s- j -1 )!} \int_0^1 \prod_{i = 0 \\ j \neq j}^{s-1} (u + i) du \text{ mit } j = 0, ...,  s-1 
$$






\subsubsection{Adam-Moulton Verfahren}

Das Adam-Moulten verfahren ist ähnlich zum
adam-bashfoth verfahren mit dem einzigen unterschied, dass das dieses implizit ist.

Das heißt die koeffizienten $b_{0}, ..., b_{s}$ sind wie folgt definiert:

$$
b_{s - j} = \frac{(-1)^j}{j!(s - j)!} \int_0^{1} \prod_{i = 0 \\ i \neq j}^{s} (u + i - 1) du, \text{ mit j = 0, ..., s}
$$
Die koeffizeinten $a_0, ..., a_s$ sind analog zum Adam-Bashforth verfahren.


\subsubsection{Backward Differentiation formeln (BDF)}

Die Backward-Diffentiation formeln interpolieren im vergleich zu dem zwei zuvor gennanten verfahren nicht 
die steigungen der einzelnen punkte sondern dessen y-Werte.
Das heißt das langrange polynom $p$ wird auf die Punkte $\{ (t_n, y_n), ..., (t_{n + s}, y_{n + s}) \}$ angewendet.
Ausserdem wird gefordert, dass gilt $p'(t_{n + s}) = f(t_{n + s}, y_{n + s})$
Dadurch ergibt sich ein nichtlineares Gleichungssystem, mit dem implizit gegebenen wert $y_{k + s}$.
Dies bedeutet, dass $b_0, ...,b_{s - 1} = 0$ sind.
Die koeffizienten $a_0, ..., a_k$ können für $s < 7$ nachgeschlagen werden.
Ist s  > 7 kann die BDF formeln nicht mehr verwendet werden, da das verfahren ab diesem zeitpunkt nicht mehr null stabil ist.


% Hinzufügen der konkreten darstellung der formeln mit dem langrage polynom















